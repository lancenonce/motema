{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We attempted three models for this hackathon. In the first attempt, we use relu and max values to determine if the input has geiger values above the threshold. we had unsupported operators, and even after removing them had difficulty. same situation with 2nd and 3rd models until we hard coded the logic in Orion in the 3rd model (see the repo).\n",
        "\n",
        "These are all the models we tried"
      ],
      "metadata": {
        "id": "zHiCR-CxhBaQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqmcoMTROKeQ",
        "outputId": "786dc73d-640d-4899-c3cf-cc45ffcc9c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rs0w2iLpRhvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.onnx\n",
        "\n",
        "class MotemaModel(nn.Module):\n",
        "    def __init__(self, threshold):\n",
        "        super(MotemaModel, self).__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Apply ReLU activation\n",
        "        relu_output = nn.functional.relu(inputs)\n",
        "\n",
        "        # Find the maximum value in the ReLU output\n",
        "        max_value, _ = torch.max(relu_output, dim=-1)\n",
        "\n",
        "        # Check if the maximum value is above threshold\n",
        "        above_threshold = (max_value > self.threshold).any()\n",
        "\n",
        "        return above_threshold\n",
        "\n",
        "# Create an instance of the model\n",
        "model = MotemaModel(threshold=0.5)\n",
        "\n",
        "# Example input tensor\n",
        "input_tensor = torch.tensor([[-1.0, 2.0, -0.5], [0.1, 0.2, 0.3]])\n",
        "\n",
        "# Export the model to ONNX\n",
        "output_path = \"motema.onnx\"\n",
        "input_names = [\"input\"]\n",
        "output_names = [\"output\"]\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    input_tensor,\n",
        "    output_path,\n",
        "    export_params=True,\n",
        "    opset_version=11,\n",
        "    do_constant_folding=True,\n",
        "    input_names=input_names,\n",
        "    output_names=output_names,\n",
        "    dynamic_axes={'input': {0: 'batch_size'},\n",
        "                  'output': {0: 'batch_size'}}\n",
        ")\n",
        "\n",
        "print(f\"Model exported to {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liM4j4U7RiSD",
        "outputId": "6ce565ca-12b2-4820-e303-b763ece00361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to motema.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnx import helper\n",
        "\n",
        "def remove_cast_nodes(model):\n",
        "    # Create a new graph with the same name and initializers\n",
        "    new_graph = helper.make_graph(\n",
        "        nodes=[],\n",
        "        name=model.graph.name,\n",
        "        inputs=model.graph.input,\n",
        "        outputs=model.graph.output,\n",
        "        initializer=model.graph.initializer\n",
        "    )\n",
        "\n",
        "    # Iterate through the original graph nodes\n",
        "    for node in model.graph.node:\n",
        "        # Skip Cast nodes\n",
        "        if node.op_type == 'Cast':\n",
        "            continue\n",
        "\n",
        "        # Create a dictionary of attributes\n",
        "        attributes = {attr.name: helper.get_attribute_value(attr) for attr in node.attribute}\n",
        "\n",
        "        # Create a new node with the same attributes\n",
        "        new_node = helper.make_node(\n",
        "            op_type=node.op_type,\n",
        "            inputs=[],\n",
        "            outputs=node.output,\n",
        "            name=node.name,\n",
        "            **attributes\n",
        "        )\n",
        "\n",
        "        # Update the input connections for the new node\n",
        "        for input_name in node.input:\n",
        "            # Check if the input is from a Cast node\n",
        "            if any(input_name in cast_node.output for cast_node in model.graph.node if cast_node.op_type == 'Cast'):\n",
        "                # Find the input node for the Cast node\n",
        "                cast_input_node = next(n for n in model.graph.node if input_name in n.output)\n",
        "                new_node.input.extend(cast_input_node.input)\n",
        "            else:\n",
        "                new_node.input.append(input_name)\n",
        "\n",
        "        # Add the new node to the new graph\n",
        "        new_graph.node.append(new_node)\n",
        "\n",
        "    # Create a new ONNX model with the updated graph\n",
        "    new_model = helper.make_model(new_graph)\n",
        "\n",
        "    return new_model\n",
        "\n",
        "# Load the existing ONNX model\n",
        "model = onnx.load('motema.onnx')\n",
        "\n",
        "# Remove Cast nodes and create a new model\n",
        "updated_model = remove_cast_nodes(model)\n",
        "\n",
        "# Save the updated ONNX model\n",
        "onnx.save(updated_model, 'new_motema.onnx')"
      ],
      "metadata": {
        "id": "OqJ1Kh9iTqPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DARNNNN! Even after removing the unsupported operations, I got an issue in Sierra that I don't have time to debug. Let's just move foward with a single layer Relu model that activates any time a value above zero is read. Hacky business."
      ],
      "metadata": {
        "id": "86xDFwEC3CBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.onnx\n",
        "\n",
        "class ThresholdReLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ThresholdReLU, self).__init__()\n",
        "        self.threshold = 10  # Hard-coded threshold value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.clamp(x, min=0, max=None) - torch.clamp(x - self.threshold, min=0, max=None) + self.threshold\n",
        "\n",
        "# Example usage\n",
        "input_tensor = torch.randn(3, 4, 5)  # Arbitrary dimension input tensor\n",
        "threshold_relu = ThresholdReLU()\n",
        "output_tensor = threshold_relu(input_tensor)\n",
        "print(output_tensor)\n",
        "\n",
        "# Export to ONNX\n",
        "dummy_input = torch.randn(1, 4, 5)  # Dummy input tensor for tracing\n",
        "onnx_path = \"threshold_relu.onnx\"\n",
        "torch.onnx.export(threshold_relu, dummy_input, onnx_path, input_names=[\"input\"], output_names=[\"output\"])\n",
        "print(f\"ONNX model exported to {onnx_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnVEx6Hz3S8V",
        "outputId": "9ed8d904-226e-4caa-f512-c7989b79d65a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[10.0000, 10.0000, 10.6845, 11.3676, 10.0000],\n",
            "         [10.0000, 10.0000, 10.0000, 10.0000, 10.9450],\n",
            "         [10.8528, 10.5069, 10.0000, 10.0000, 10.0000],\n",
            "         [10.0000, 10.0000, 10.0000, 12.7550, 10.5501]],\n",
            "\n",
            "        [[10.7883, 10.0000, 10.0000, 10.6433, 10.5456],\n",
            "         [11.7491, 10.6858, 10.1643, 11.2402, 11.0839],\n",
            "         [10.0000, 11.6770, 10.4200, 10.1146, 10.0000],\n",
            "         [10.3730, 11.1745, 10.0624, 11.0636, 10.0000]],\n",
            "\n",
            "        [[10.2130, 10.6126, 10.4162, 10.0000, 10.0000],\n",
            "         [10.4675, 10.0000, 10.0000, 10.3210, 10.0000],\n",
            "         [11.0580, 10.8105, 11.0548, 10.0000, 10.0000],\n",
            "         [10.3731, 11.4597, 10.0000, 11.0546, 10.0000]]])\n",
            "ONNX model exported to threshold_relu.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drat. Soiled again. We can't use clamp. YOLO SINGLE LAYER RELU"
      ],
      "metadata": {
        "id": "7rgKUCO36N1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.onnx\n",
        "\n",
        "class ThresholdReLU(nn.Module):\n",
        "    def __init__(self, threshold=10):\n",
        "        super(ThresholdReLU, self).__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.max(x, torch.zeros_like(x)) - torch.max(x - self.threshold, torch.zeros_like(x)) + self.threshold\n",
        "\n",
        "# Example usage\n",
        "input_tensor = torch.tensor([-5.0, 2.0, 8.0, -3.0, 12.0, 0.0, 7.0, -1.0, 10.0, 6.0])\n",
        "threshold_relu = ThresholdReLU(threshold=10)\n",
        "output_tensor = threshold_relu(input_tensor)\n",
        "print(output_tensor)\n",
        "\n",
        "# Export to ONNX\n",
        "dummy_input = torch.randn(10)  # Dummy input tensor for tracing\n",
        "onnx_path = \"threshold_relu_two.onnx\"\n",
        "torch.onnx.export(threshold_relu, dummy_input, onnx_path, input_names=[\"input\"], output_names=[\"output\"])\n",
        "print(f\"ONNX model exported to {onnx_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J4vlV706RVJ",
        "outputId": "ec77a228-4d2c-4e55-d8b7-9c77f6fe9b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10., 12., 18., 10., 20., 10., 17., 10., 20., 16.])\n",
            "ONNX model exported to threshold_relu_two.onnx\n"
          ]
        }
      ]
    }
  ]
}